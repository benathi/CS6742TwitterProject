\relax 
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {I-A}}Motivation}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Example of Code-Switching Tweets\relax }}{1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:ex_csTweets}{{1}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {I-B}}Data Choice}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {I-C}}Related Work}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {II}High Level Summary}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {III}Methodology}{1}}
\newlabel{fig:process}{{\caption@xref {fig:process}{ on input line 583}}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Data Pre-Processing\relax }}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-A}}Filtering Duplicates}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-B}}Phrase Segmentation}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-C}}Word Breaking (Thai) }{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-D}}Obtaining N-Grams}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-E}}Design Matrix and Labels}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-F}}Part of Speech Analysis}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-G}}Punctuation Filtering}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-H}}CS Labeling}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-I}}CS Words Analysis}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-J}}Regression Label}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Results}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-A}}Part of Speech of Code-Switching Words}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Distribution of Part of Speech for Words in Code Switching Phrases vs Words in English Tweets \relax }}{3}}
\newlabel{fig:pos_phrases}{{3}{3}}
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces Part of Speech Legend and Examples. See ~\ref  {bib:GimpelEtal} for more details.\relax }}{3}}
\newlabel{tab:pos_legend}{{I}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Histogram for number of words in pseudo code-switching phrases. There are $775,345$ pseudo code-switching phrases contained in $102,550$ Tweets.\relax }}{3}}
\newlabel{fig:hist_cs_len}{{4}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Distribution of Part of Speech for Words in Code Switching Unigrams vs Words in English Tweets\relax }}{4}}
\newlabel{fig:pos_uni}{{5}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-B}}Code-Switching Unigrams}{4}}
\@writefile{lot}{\contentsline {table}{\numberline {II}{\ignorespaces List of Proper Nouns for Manual Filter\relax }}{4}}
\newlabel{tab:properNouns}{{II}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Distribution of part of speech for words in code switching unigrams versus words in English tweets. The second plot is the original plot but shows only the lower middle region of the original plot. \relax }}{4}}
\newlabel{fig:cs_words}{{6}{4}}
\newlabel{fig:test1}{{7a}{5}}
\newlabel{sub@fig:test1}{{(a)}{a}}
\newlabel{fig:test2}{{7b}{5}}
\newlabel{sub@fig:test2}{{(b)}{b}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Distribution of Part of Speech for Words in Code Switching Unigrams vs Words in English Tweets\relax }}{5}}
\newlabel{fig:pos_uni_goodPOS}{{7}{5}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Distribution of POS for Code-Switching Words}}}{5}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Distribution of POS for English Words}}}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-C}}Logistic Regression}{5}}
\@writefile{lot}{\contentsline {table}{\numberline {III}{\ignorespaces Average of prediction scores of 5-fold cross validations for both lasso and ridge regression. The predication score is defined by the percentage of correct prediction.\relax }}{5}}
\newlabel{tab:predictionScores}{{III}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-D}}Significant Thai N-Grams}{5}}
\@writefile{lot}{\contentsline {table}{\numberline {IV}{\ignorespaces Prediction scores for the baseline estimator (predict all non code-switch) and the classified trained by regularized logistic regression. \relax }}{5}}
\newlabel{tab:predictionScores}{{IV}{5}}
\@writefile{lot}{\contentsline {table}{\numberline {V}{\ignorespaces List of Thai $1,2,3$-Gram with Postive Betas\relax }}{5}}
\newlabel{tab:significantNgram}{{V}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Discussion}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {V-A}}Code-Switching Determination}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Example Tweet\relax }}{6}}
\newlabel{fig:GS_compare_column_alpha}{{8}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {V-B}}Sparsity of Code Switching Data}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {VI}Conclusion}{6}}
\bibcite{IEEEhowto:kopka}{1}
\bibcite{bib:GimpelEtal}{2}
\bibcite{bib:BrownCluster}{3}
\@writefile{toc}{\contentsline {section}{References}{7}}
